<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="author" content="Frank Coelho de Alcantara -2020" />
  <title>Word2Vector</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta
    name="apple-mobile-web-app-status-bar-style"
    content="black-translucent" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" />
  <link rel="stylesheet" href="../../rev/reset.css" />
  <link rel="stylesheet" href="../../rev/reveal.css" />
  <link rel="stylesheet" href="../../rev/interpret.css" />
</head>

<body>
  <div class="reveal">
    <div class="slides">
      <section id="title-slide" class="nivel1">
        <section>
          <h1 class="title" style="width: 90%; margin-left: 4%; font-size: 340%;">Word2Vector</h1>
          <p style="text-align: right !important;">Frank Coelho de Alcantara -2020&nbsp;&nbsp;</p>
        </section>
        <section>
            <h2>Vetorização</h2>
            <small style="font-size: 73% !important; margin-left: 0%; width: 95%;">
              <p  class="fragment fade-up">Algoritmo de aprendizagem supervisionada para a vetorização criado em 2013.</p>
              <p  class="fragment fade-up">Mikolov, Tomas; et al. (2013). "Efficient Estimation of Word Representations in Vector Space". <a href="https://arxiv.org/archive/cs.CL" target="_blank" rel="noopener noreferrer">arXiv:1301.3781</a>.</p>
              <p  class="fragment fade-up">Mikolov, Tomas (2013). "Distributed representations of words and phrases and their compositionality". Advances in Neural Information Processing Systems. <a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener noreferrer">arXiv:1310.4546</a>.</p>
              <p  class="fragment fade-up">O objetivo é criar vetores menos esparsos e mais uteis para descobrir a relação entre termos em documentos.</p>
            </small>
        </section>
        <section>
            <h2>Matriz Esparsa</h2>
            <small style="font-size: 73% !important; margin-left: 0%; width: 95%;">
              <p  class="fragment fade-up">Já vimos que a vetorização cria matrizes que relacionam termos e documentos de uma forma esparsa.</p>
              <p  class="fragment fade-up">Ainda que exista informação nesta representação, perdemos semântica e criamos grandes matrizes cheias de zeros.</p>
              <p  class="fragment fade-up">Por outro lado, criamos vetores que representam documentos em um espaço multidimensional onde cada termo é uma dimensão.</p>
              <p  class="fragment fade-up">Podemos ver a relação entre os vetores de cada documento usando a distância entre eles.</p>
              <p  class="fragment fade-up">Dois documentos exatamente iguais terão o mesmo comprimento e o ângulo entre eles será zero. O ângulo é importante. Muito importante.</p>
            </small>
        </section>
        <section>
            <h2><i>Cossine Similarity</i></h2>
            <small style="font-size: 54% !important; margin-left: -2%; width: 49%;">
              <p  class="fragment fade-up">Métrica utilizada para determinar o grau de similaridade entre dois documentos independente dos seus tamanhos.</p>
              <p  class="fragment fade-up">Mediremos o cosseno do ângulo entre dois vetores em um espaço multidimensional.</p>
              <p  class="fragment fade-up">No nosso caso, dois vetores são estruturas de dados contendo informações sobre os dois documentos.</p>
              <p  class="fragment fade-up">Cada termo corresponde a uma dimensão e a quantidade destes termos indica o escalar da dimensão. E podemos determinar a orientação do vetor que representa o texto.</p>
              <p  class="fragment fade-up">Usando o cosseno entre os ângulos podemos criar uma medida de similaridade entre os vetores criados independente do comprimento destes vetores.</p>
            </small>
            <img data-src="../img/w2v1.jpg" class="fragment fade-up" style="margin-right: 1%; height: 360px !important; float: right;"  alt="gráfico mostrando um vetor entre a origem e o ponto (1,2,3)." />
        </section>
        <section>
            <h2>Cossine Similarity - Matemática</h2>
            <small style="font-size: 61% !important; margin-left: 0%; width: 97%;">
              <p  class="fragment fade-up">A Similaridade entre dois vetores, $A$ e $B$ será dada por: 
                $$cos(\theta) = \frac{A\cdot B}{|A||B|} $$ </p>
              <p  class="fragment fade-up">Ou, considerando as múltiplas dimensões teremos: $$cos(\theta)= \frac{\sum_{i=1}^{n}A_i B_i}{\sqrt{\sum_{i=1}^{n}A_i} \sqrt{\sum_{i=1}^{n}A_i}}$$</p>
              <p  class="fragment fade-up">O qué bastante simples usando o <i>Numpy</i> $$cos\_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))$$</p>
            </small>
        </section>
        <section>
            <h2>Word2Vector - CBOW</h2>
            <small style="float: left; font-size: 61% !important; margin-left: 0%; width: 50%;">
              <p  class="fragment fade-up">Word2Vector usa dois algoritmos para a criação do vetor: CBOW e SKIP-GRAM</p>
              <p  class="fragment fade-up"><strong>CBOW</strong> <i>Common Bag Of Words</i>: considera o contexto de um determinado termo e tenta inferir 
              que termo é este.</p>
              <p  class="fragment fade-up">Funciona com a determinação de uma janela, um conjunto de palavras e dentro desta janela, centraliza a palavra desejada.</p>
              <p  class="fragment fade-up">Trata-se de um algoritmo de aprendizagem supervisionada consistindo de uma rede neural com três camadas com apenas uma camada oculta.</p>
              <p  class="fragment fade-up">Você pode ver uma implementação deste algoritmo em python, usando o Keras <a href="https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/" target="_blank" rel="noopener noreferrer">aqui!</a></p>
            </small>
            <img data-src="../img/w2v2.jpg" class="fragment fade-up" style="margin-right: 1%; height: 360px !important; float: right;"  alt="diagrama em blocos de uma rede neural apresentando três camadas." />
        </section>
        <section>
            <h2>Word2Vector - SKIP-GRAM</h2>
            <small style="float: left; font-size: 61% !important; margin-left: 0%; width: 50%;">
              <p  class="fragment fade-up"><strong>SKIP-GRAM</strong>: partindo de um termo determinado o Skip-Gram tenta determinar os termos que rodeiam este termo.</p>
              <p  class="fragment fade-up">Funciona com a determinação de uma janela, e vai criar os termos que estão dentro desta janela em torno do termo chave. É o processo inverso ao CBOW</p>
              <p  class="fragment fade-up">Trata-se de um algoritmo de aprendizagem supervisionada consistindo de uma rede neural com três camadas com apenas uma camada oculta.</p>
              <p  class="fragment fade-up">Você pode ver uma implementação deste algoritmo em python <a href="https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/" target="_blank" rel="noopener noreferrer">aqui!</a></p>
            </small>
            <img data-src="../img/w2v3.jpg" class="fragment fade-up" style="margin-right: 1%; height: 360px !important; float: right;"  alt="diagrama em blocos de uma rede neural apresentando três camadas." />
        </section>
        <section>
            <h2>Word2Vector - Completo</h2>
            <small style="float: left; font-size: 61% !important; margin-left: 0%; width: 50%;">
              <p  class="fragment fade-up">o <strong>Word2Vector</strong> usa estes dois algoritmos com a intensão de criar um vetor a partir de um determinado corpus. </p>
              <p  class="fragment fade-up">Tanto o CBOW quanto o SKIP-GRAM são usados apenas para descobrir as informações que criarão um vetor mais próximo de algum sentido semântico.</p>
              <p  class="fragment fade-up">A dimensão do vetor é o número de features que temos na saída.</p>
              <p  class="fragment fade-up">Esta imagem está disponível <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
                " target="_blank" rel="noopener noreferrer">aqui!</a></p>
            </small>
            <img data-src="../img/w2v1.png" class="fragment fade-up" style="margin-right: 1%; height: 295px !important; float: right;"  alt="diagrama em blocos de uma rede neural apresentando três camadas." />
        </section>
        <section>
            <h2>Exercício 1</h2>
            <small style="font-size: 73% !important; margin-left: 0%; width: 97%;">
              <p  class="fragment fade-up">Seu trabalho será, usando o Google Colab, determinar a similaridade, usando <i>Cossine Similarity</i> entre todos os  documentos formados por um corpus de dois arquivos científicos, quaisquer, com mais de 15 páginas em PDF, escritos em inglês e convertidos para txt com a ferramenta que você desejar.</p>
              <p  class="fragment fade-up">O resultado deve ser uma matriz relacionando estes parágrafos e a sua similaridade.</p>
              <p  class="fragment fade-up">Este trabalho vale a presença para as aulas de 11 de novembro e 18 de novembro.</p>
            </small>
        </section>
        <section>
            <h2>Exercício 2</h2>
            <small style="font-size: 73% !important; margin-left: 0%; width: 97%;">
              <p  class="fragment fade-up">Seu trabalho será, usando o Google Colab, fazer uma implementação, <i>from scratch</i> do CBOW.</p>
              <p  class="fragment fade-up">O resultado deve ser uma rede neural que, dado um conjunto de 4 palavras indique a palavra mais provável de estar 
                exatamente no meio. Treine seu algoritmo com os textos do Machado de Assis disponíveis na NLTK</p>
              <p  class="fragment fade-up">Este trabalho vale a presença para as aulas de 11 de novembro e 18 de novembro.</p>
            </small>
        </section>
        <section>
            <h2>Exercício 2</h2>
            <small style="font-size: 73% !important; margin-left: 0%; width: 97%;">
              <p  class="fragment fade-up">Seu trabalho será, usando o Google Colab, fazer uma implementação, <i>from scratch</i> do SKIP-GRAM.</p>
              <p  class="fragment fade-up">O resultado deve ser uma rede neural que, dado um termo indique as quatro palavras mais prováveis de estar 
                circundando este termo. Treine seu algoritmo com os textos do Machado de Assis disponíveis na NLTK</p>
              <p  class="fragment fade-up">Este trabalho vale a presença para as aulas de 11 de novembro e 18 de novembro.</p>
            </small>
        </section>
      </section>
    </div>
  </div>
  <div class="home-button"><a href="https://frankalcantara.com"><i class="fas fa-home"></i></a></div>
  <script src="../../rev/reveal.js"></script>
  <script src="../../rev/plugin/notes/notes.js"></script>
  <script src="../../rev/plugin/search/search.js"></script>
  <script src="../../rev/plugin/zoom/zoom.js"></script>
  <script src="../../rev/plugin/math/math.js"></script>
  <script src="../../rev/plugin/menu/menu.js"></script>
  <script src="../../rev/plugin/chalkboard/plugin.js"></script>

  <script>
    // Full list of configuration options available at:
    // https://revealjs.com/config/
    Reveal.initialize({
      // Push each slide change to the browser history
      history: true,
      // Transition style
      transition: "fade", // none/fade/slide/convex/concave/zoom
      center: false,
      math: {
        mathjax:
          "https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js",
        config: "TeX-AMS_HTML-full",
        // pass other options into `MathJax.Hub.Config()`
        TeX: {
          Macros: {
            RR: "{\\bf R}",
          },
        },
      },
      menu: {
        side: "left",
        width: "normal",
        numbers: false,
        titleSelector: "h1, h2, h3, h4, h5, h6",
        useTextContentForMissingTitles: false,
        hideMissingTitles: false,
        markers: true,
        custom: false,
        themes: false,
        themesPath: "dist/theme/",
        transitions: false,
        openButton: true,
        openSlideNumber: false,
        keyboard: true,
        sticky: false,
        autoOpen: true,
        delayInit: false,
        openOnInit: false,
        loadIcons: true,
      },

      // reveal.js plugins
      plugins: [
        RevealNotes,
        RevealMath,
        RevealMenu,
        RevealChalkboard,
        RevealSearch,
        RevealZoom,
      ],
    });
  </script>
</body>

</html>